Lab 4 Writeup
Jason Hoch
4/29/2013

I would like to use one (1) late day on this assignment.  I implemented
one of the optional extra credit improvements (see below).

Describe any design decisions you made.

The design decisions I made were fairly minimal.  For the IntHistogram
implementation, I store an array of how many values have been added to each
bucket (the "heights"), as well as the total number of values that have been
added.  In general, to estimate selectivity, the approximate
number of "sufficient values," i.e. values that would satisfy the query, is
found.  For example, in the case of an "equals v" query, the height of the
bucket containing v is divided by its width to get the number of sufficient
values.  In all cases, the number of sufficient values is divided by the total
number of values in order to estimate the selectivity.  Bucket widths are
pre-computed and stored in an array, as are the minimum values stored in each
bucket; these minimums are used in "less than" and "greater than" queries,
because the number of values less than v in a bucket is estimated to be
(v - minimum)/bucketWidth * bucketHeight.  These approximations are all the best
that can be done while storing simple heights of each bucket.  Running time
is O(number of buckets), since for greater than and less than queries it
might be necessary to take a sum across O(n) buckets.  While this does not
scale with the number of added values, it is not optimal, since it's possible
to store sums across buckets in a tree and determine range queries in
O(log(number of buckets)) time; however, the reduced time complexity did not
merit the increased code complexity in this case.

Join cardinality estimations were done in the following way: primary key
equality conditions took the minimum cardinality of the two children result
sets, or the cardinality of the non-primary key set if there was only one
primary key.  Equality conditions without primary keys took the maximum
cardinality.  Inequality conditions took the product of the two cardinalities,
estimating that on the order of very few pairs of tuples would be eliminated.
Greater than and less than queries were assumed to on average eliminate 50%
of candidate tuples, so the returned estimated cardinality was 0.5 * the
product of the two cardinalities.

Join ordering was done using the provided helper methods, following the
pseudocode in the lab description.

For extra credit I implemented an improvement to the subset enumerator.
Instead of generating all of the subsets at once, I implemented a "bitset"
iterator that could in turn be used to create a subset iterator.  I put
"bitset" in quotation marks because the Bitset class was added in Java 7,
so in Java 6 I just used a List<Integer>.  The API was the same - it only
iterates through subsets of a specified size.  This allowed much faster
orderings of large join operations:

[junit] Testcase: estimateJoinCostTest took 1.216 sec
[junit] Testcase: estimateJoinCardinality took 0.135 sec
[junit] Testcase: orderJoinsTest took 1.742 sec
[junit] Testcase: bigOrderJoinsTest took 1.449 sec
[junit] Testcase: reallyBigOrderJoinsTest took 16.243 sec
[junit] Testcase: nonequalityOrderJoinsTest took 0.12 sec

The "reallyBigOrderJoinsTest" case above combines twenty tables, and runs
quite fast compared to the estimated unoptimized time (on the order of minutes
or hours according to the lab description).

Discuss and justify any changes you made to the API.

No changes were made to the API.

Describe any missing or incomplete elements of your code.

I didn't implement average selectivity, or an optional toString() method.

Describe how long you spent on the lab, and whether there was anything you
found particularly difficult or confusing.

I spent about 12 hours on the lab, and spent more time debugging than usual.
For some reason, the IntHistogram provided me recurring trouble, especially
in the case that there were more buckets than possible integer values.  I found
Selinger's algorithm's implementation hard to wrap my head around, but the
provided helper methods ended up making it pretty simple.
